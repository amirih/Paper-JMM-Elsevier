	.file	"IMP1.c"
	.text
	.p2align 4,,15
	.globl	assignToThisCore12
	.type	assignToThisCore12, @function
assignToThisCore12:
.LFB5460:
	.cfi_startproc
	movl	%edi, %r8d
	movl	$mask, %edx
	movl	$16, %ecx
	xorl	%eax, %eax
	movq	%rdx, %rdi
	rep stosq
	movslq	%r8d, %rax
	cmpq	$1023, %rax
	ja	.L2
	shrq	$6, %rax
	movl	$1, %edx
	shlx	%r8, %rdx, %r8
	orq	%r8, mask(,%rax,8)
.L2:
	movslq	%esi, %rax
	cmpq	$1023, %rax
	ja	.L3
	shrq	$6, %rax
	movl	$1, %edx
	shlx	%rsi, %rdx, %rsi
	orq	%rsi, mask(,%rax,8)
.L3:
	movl	$mask, %edx
	movl	$128, %esi
	xorl	%edi, %edi
	jmp	sched_setaffinity
	.cfi_endproc
.LFE5460:
	.size	assignToThisCore12, .-assignToThisCore12
	.p2align 4,,15
	.globl	assignImagef32
	.type	assignImagef32, @function
assignImagef32:
.LFB5461:
	.cfi_startproc
	pushq	%rbx
	.cfi_def_cfa_offset 16
	.cfi_offset 3, -16
	movq	%rdi, %rbx
	xorl	%r9d, %r9d
	vmovsd	.LC0(%rip), %xmm3
	vmovsd	.LC1(%rip), %xmm5
	vmovsd	.LC2(%rip), %xmm4
	movl	$1195121335, %edi
	.p2align 4,,10
	.p2align 3
.L6:
	vcvtsi2sd	%r9d, %xmm2, %xmm2
	vfmadd132sd	%xmm5, %xmm4, %xmm2
	movq	%rbx, %r11
	movl	%r9d, %r10d
	xorl	%esi, %esi
	.p2align 4,,10
	.p2align 3
.L7:
	leal	1(%rsi), %r8d
	movl	%r9d, %eax
	cltd
	idivl	%r8d
	leal	(%rax,%r10), %ecx
	movl	%ecx, %edx
	shrl	%edx
	movl	%edx, %eax
	mull	%edi
	shrl	$5, %edx
	imull	$230, %edx, %edx
	subl	%edx, %ecx
	vcvtsi2sd	%ecx, %xmm0, %xmm0
	vcvtsi2sd	%esi, %xmm1, %xmm1
	vfmadd132sd	%xmm3, %xmm2, %xmm1
	vaddsd	%xmm1, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm6, %xmm6
	vmovss	%xmm6, (%r11)
	addl	%r9d, %r10d
	addq	$4, %r11
	movl	%r8d, %esi
	cmpl	$64, %r8d
	jne	.L7
	incl	%r9d
	addq	$272, %rbx
	cmpl	$64, %r9d
	jne	.L6
	popq	%rbx
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE5461:
	.size	assignImagef32, .-assignImagef32
	.p2align 4,,15
	.globl	assignMatrixf32
	.type	assignMatrixf32, @function
assignMatrixf32:
.LFB5462:
	.cfi_startproc
	xorl	%r8d, %r8d
	vmovsd	.LC2(%rip), %xmm1
	movl	$274877907, %r10d
	.p2align 4,,10
	.p2align 3
.L13:
	xorl	%r9d, %r9d
	movl	$1, %esi
	.p2align 4,,10
	.p2align 3
.L14:
	movl	%r8d, %eax
	cltd
	idivl	%esi
	leal	(%rax,%r9), %ecx
	movl	%ecx, %eax
	mull	%r10d
	shrl	$6, %edx
	imull	$1000, %edx, %edx
	subl	%edx, %ecx
	vcvtsi2sd	%ecx, %xmm0, %xmm0
	vaddsd	%xmm1, %xmm0, %xmm0
	vcvtsd2ss	%xmm0, %xmm2, %xmm2
	vmovss	%xmm2, -4(%rdi,%rsi,4)
	incq	%rsi
	addl	%r8d, %r9d
	cmpq	$65, %rsi
	jne	.L14
	incl	%r8d
	addq	$256, %rdi
	cmpl	$64, %r8d
	jne	.L13
	ret
	.cfi_endproc
.LFE5462:
	.size	assignMatrixf32, .-assignMatrixf32
	.p2align 4,,15
	.globl	assignImagei32
	.type	assignImagei32, @function
assignImagei32:
.LFB5463:
	.cfi_startproc
	xorl	%r8d, %r8d
	movl	$-2139062143, %r10d
	.p2align 4,,10
	.p2align 3
.L19:
	xorl	%r9d, %r9d
	movl	$1, %esi
	.p2align 4,,10
	.p2align 3
.L20:
	movl	%r8d, %eax
	cltd
	idivl	%esi
	leal	(%rax,%r9), %ecx
	movl	%ecx, %eax
	mull	%r10d
	shrl	$7, %edx
	movl	%edx, %eax
	sall	$8, %eax
	subl	%edx, %eax
	subl	%eax, %ecx
	movl	%ecx, -4(%rdi,%rsi,4)
	incq	%rsi
	addl	%r8d, %r9d
	cmpq	$65, %rsi
	jne	.L20
	incl	%r8d
	addq	$272, %rdi
	cmpl	$64, %r8d
	jne	.L19
	ret
	.cfi_endproc
.LFE5463:
	.size	assignImagei32, .-assignImagei32
	.p2align 4,,15
	.globl	assignMatrixi32
	.type	assignMatrixi32, @function
assignMatrixi32:
.LFB5464:
	.cfi_startproc
	xorl	%eax, %eax
	vmovdqa	.LC3(%rip), %ymm12
	vmovdqa	.LC4(%rip), %ymm0
	vmovdqa	.LC5(%rip), %ymm4
	vmovdqa	.LC6(%rip), %ymm3
	vmovdqa	.LC7(%rip), %ymm11
	vmovdqa	.LC8(%rip), %ymm10
	vmovdqa	.LC9(%rip), %ymm9
	vmovdqa	.LC10(%rip), %ymm8
	vmovdqa	.LC11(%rip), %ymm7
	vmovdqa	.LC12(%rip), %ymm6
	vmovdqa	.LC13(%rip), %ymm5
	.p2align 4,,10
	.p2align 3
.L25:
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpmulld	%ymm12, %ymm1, %ymm13
	vpsrlq	$32, %ymm13, %ymm14
	vpmuldq	%ymm0, %ymm13, %ymm2
	vpmuldq	%ymm0, %ymm14, %ymm14
	vpshufb	%ymm4, %ymm2, %ymm2
	vpshufb	%ymm3, %ymm14, %ymm14
	vpor	%ymm14, %ymm2, %ymm2
	vpsrad	$6, %ymm2, %ymm14
	vpslld	$5, %ymm14, %ymm2
	vpsubd	%ymm14, %ymm2, %ymm2
	vpslld	$2, %ymm2, %ymm2
	vpaddd	%ymm14, %ymm2, %ymm2
	vpslld	$3, %ymm2, %ymm2
	vpsubd	%ymm2, %ymm13, %ymm2
	vmovdqu	%ymm2, (%rdi)
	vpmulld	%ymm11, %ymm1, %ymm13
	vpsrlq	$32, %ymm13, %ymm14
	vpmuldq	%ymm0, %ymm13, %ymm2
	vpmuldq	%ymm0, %ymm14, %ymm14
	vpshufb	%ymm4, %ymm2, %ymm2
	vpshufb	%ymm3, %ymm14, %ymm14
	vpor	%ymm14, %ymm2, %ymm2
	vpsrad	$6, %ymm2, %ymm14
	vpslld	$5, %ymm14, %ymm2
	vpsubd	%ymm14, %ymm2, %ymm2
	vpslld	$2, %ymm2, %ymm2
	vpaddd	%ymm14, %ymm2, %ymm2
	vpslld	$3, %ymm2, %ymm2
	vpsubd	%ymm2, %ymm13, %ymm2
	vmovdqu	%ymm2, 32(%rdi)
	vpmulld	%ymm10, %ymm1, %ymm13
	vpsrlq	$32, %ymm13, %ymm14
	vpmuldq	%ymm0, %ymm13, %ymm2
	vpmuldq	%ymm0, %ymm14, %ymm14
	vpshufb	%ymm4, %ymm2, %ymm2
	vpshufb	%ymm3, %ymm14, %ymm14
	vpor	%ymm14, %ymm2, %ymm2
	vpsrad	$6, %ymm2, %ymm14
	vpslld	$5, %ymm14, %ymm2
	vpsubd	%ymm14, %ymm2, %ymm2
	vpslld	$2, %ymm2, %ymm2
	vpaddd	%ymm14, %ymm2, %ymm2
	vpslld	$3, %ymm2, %ymm2
	vpsubd	%ymm2, %ymm13, %ymm2
	vmovdqu	%ymm2, 64(%rdi)
	vpmulld	%ymm9, %ymm1, %ymm13
	vpsrlq	$32, %ymm13, %ymm14
	vpmuldq	%ymm0, %ymm13, %ymm2
	vpmuldq	%ymm0, %ymm14, %ymm14
	vpshufb	%ymm4, %ymm2, %ymm2
	vpshufb	%ymm3, %ymm14, %ymm14
	vpor	%ymm14, %ymm2, %ymm2
	vpsrad	$6, %ymm2, %ymm14
	vpslld	$5, %ymm14, %ymm2
	vpsubd	%ymm14, %ymm2, %ymm2
	vpslld	$2, %ymm2, %ymm2
	vpaddd	%ymm14, %ymm2, %ymm2
	vpslld	$3, %ymm2, %ymm2
	vpsubd	%ymm2, %ymm13, %ymm2
	vmovdqu	%ymm2, 96(%rdi)
	vpmulld	%ymm8, %ymm1, %ymm13
	vpsrlq	$32, %ymm13, %ymm14
	vpmuldq	%ymm0, %ymm13, %ymm2
	vpmuldq	%ymm0, %ymm14, %ymm14
	vpshufb	%ymm4, %ymm2, %ymm2
	vpshufb	%ymm3, %ymm14, %ymm14
	vpor	%ymm14, %ymm2, %ymm2
	vpsrad	$6, %ymm2, %ymm14
	vpslld	$5, %ymm14, %ymm2
	vpsubd	%ymm14, %ymm2, %ymm2
	vpslld	$2, %ymm2, %ymm2
	vpaddd	%ymm14, %ymm2, %ymm2
	vpslld	$3, %ymm2, %ymm2
	vpsubd	%ymm2, %ymm13, %ymm2
	vmovdqu	%ymm2, 128(%rdi)
	vpmulld	%ymm7, %ymm1, %ymm13
	vpsrlq	$32, %ymm13, %ymm14
	vpmuldq	%ymm0, %ymm13, %ymm2
	vpmuldq	%ymm0, %ymm14, %ymm14
	vpshufb	%ymm4, %ymm2, %ymm2
	vpshufb	%ymm3, %ymm14, %ymm14
	vpor	%ymm14, %ymm2, %ymm2
	vpsrad	$6, %ymm2, %ymm14
	vpslld	$5, %ymm14, %ymm2
	vpsubd	%ymm14, %ymm2, %ymm2
	vpslld	$2, %ymm2, %ymm2
	vpaddd	%ymm14, %ymm2, %ymm2
	vpslld	$3, %ymm2, %ymm2
	vpsubd	%ymm2, %ymm13, %ymm2
	vmovdqu	%ymm2, 160(%rdi)
	vpmulld	%ymm6, %ymm1, %ymm13
	vpsrlq	$32, %ymm13, %ymm14
	vpmuldq	%ymm0, %ymm13, %ymm2
	vpmuldq	%ymm0, %ymm14, %ymm14
	vpshufb	%ymm4, %ymm2, %ymm2
	vpshufb	%ymm3, %ymm14, %ymm14
	vpor	%ymm14, %ymm2, %ymm2
	vpsrad	$6, %ymm2, %ymm14
	vpslld	$5, %ymm14, %ymm2
	vpsubd	%ymm14, %ymm2, %ymm2
	vpslld	$2, %ymm2, %ymm2
	vpaddd	%ymm14, %ymm2, %ymm2
	vpslld	$3, %ymm2, %ymm2
	vpsubd	%ymm2, %ymm13, %ymm2
	vmovdqu	%ymm2, 192(%rdi)
	vpmulld	%ymm5, %ymm1, %ymm1
	vpsrlq	$32, %ymm1, %ymm13
	vpmuldq	%ymm0, %ymm1, %ymm2
	vpmuldq	%ymm0, %ymm13, %ymm13
	vpshufb	%ymm4, %ymm2, %ymm2
	vpshufb	%ymm3, %ymm13, %ymm13
	vpor	%ymm13, %ymm2, %ymm2
	vpsrad	$6, %ymm2, %ymm13
	vpslld	$5, %ymm13, %ymm2
	vpsubd	%ymm13, %ymm2, %ymm2
	vpslld	$2, %ymm2, %ymm2
	vpaddd	%ymm13, %ymm2, %ymm2
	vpslld	$3, %ymm2, %ymm2
	vpsubd	%ymm2, %ymm1, %ymm1
	vmovdqu	%ymm1, 224(%rdi)
	incl	%eax
	addq	$256, %rdi
	cmpl	$64, %eax
	jne	.L25
	vzeroupper
	ret
	.cfi_endproc
.LFE5464:
	.size	assignMatrixi32, .-assignMatrixi32
	.p2align 4,,15
	.globl	assignMatrixi16
	.type	assignMatrixi16, @function
assignMatrixi16:
.LFB5465:
	.cfi_startproc
	pushq	%r14
	.cfi_def_cfa_offset 16
	.cfi_offset 14, -16
	pushq	%r13
	.cfi_def_cfa_offset 24
	.cfi_offset 13, -24
	pushq	%r12
	.cfi_def_cfa_offset 32
	.cfi_offset 12, -32
	pushq	%rbp
	.cfi_def_cfa_offset 40
	.cfi_offset 6, -40
	pushq	%rbx
	.cfi_def_cfa_offset 48
	.cfi_offset 3, -48
	leaq	128(%rdi), %rbx
	xorl	%ebp, %ebp
	movl	$558694933, %r12d
	.p2align 4,,10
	.p2align 3
.L28:
	leaq	-128(%rbx), %r14
	xorl	%r13d, %r13d
	.p2align 4,,10
	.p2align 3
.L29:
	call	rand
	movl	%eax, %ecx
	movl	%r13d, %edx
	shrl	%edx
	movl	%edx, %eax
	mull	%r12d
	shrl	$4, %edx
	imull	$246, %edx, %edx
	movl	%r13d, %esi
	subl	%edx, %esi
	movl	$1717986919, %edx
	movl	%ecx, %eax
	imull	%edx
	sarl	$2, %edx
	movl	%ecx, %eax
	sarl	$31, %eax
	subl	%eax, %edx
	leal	(%rdx,%rdx,4), %eax
	addl	%eax, %eax
	subl	%eax, %ecx
	addl	%esi, %ecx
	movw	%cx, (%r14)
	addl	%ebp, %r13d
	addq	$2, %r14
	cmpq	%r14, %rbx
	jne	.L29
	incl	%ebp
	subq	$-128, %rbx
	cmpl	$64, %ebp
	jne	.L28
	popq	%rbx
	.cfi_def_cfa_offset 40
	popq	%rbp
	.cfi_def_cfa_offset 32
	popq	%r12
	.cfi_def_cfa_offset 24
	popq	%r13
	.cfi_def_cfa_offset 16
	popq	%r14
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE5465:
	.size	assignMatrixi16, .-assignMatrixi16
	.p2align 4,,15
	.globl	assignImagei16
	.type	assignImagei16, @function
assignImagei16:
.LFB5489:
	.cfi_startproc
	pushq	%r14
	.cfi_def_cfa_offset 16
	.cfi_offset 14, -16
	pushq	%r13
	.cfi_def_cfa_offset 24
	.cfi_offset 13, -24
	pushq	%r12
	.cfi_def_cfa_offset 32
	.cfi_offset 12, -32
	pushq	%rbp
	.cfi_def_cfa_offset 40
	.cfi_offset 6, -40
	pushq	%rbx
	.cfi_def_cfa_offset 48
	.cfi_offset 3, -48
	leaq	128(%rdi), %rbx
	xorl	%ebp, %ebp
	movl	$558694933, %r12d
	.p2align 4,,10
	.p2align 3
.L35:
	leaq	-128(%rbx), %r14
	xorl	%r13d, %r13d
	.p2align 4,,10
	.p2align 3
.L36:
	call	rand
	movl	%eax, %ecx
	movl	%r13d, %edx
	shrl	%edx
	movl	%edx, %eax
	mull	%r12d
	shrl	$4, %edx
	imull	$246, %edx, %edx
	movl	%r13d, %esi
	subl	%edx, %esi
	movl	$1717986919, %edx
	movl	%ecx, %eax
	imull	%edx
	sarl	$2, %edx
	movl	%ecx, %eax
	sarl	$31, %eax
	subl	%eax, %edx
	leal	(%rdx,%rdx,4), %eax
	addl	%eax, %eax
	subl	%eax, %ecx
	addl	%esi, %ecx
	movw	%cx, (%r14)
	addl	%ebp, %r13d
	addq	$2, %r14
	cmpq	%r14, %rbx
	jne	.L36
	incl	%ebp
	subq	$-128, %rbx
	cmpl	$64, %ebp
	jne	.L35
	popq	%rbx
	.cfi_def_cfa_offset 40
	popq	%rbp
	.cfi_def_cfa_offset 32
	popq	%r12
	.cfi_def_cfa_offset 24
	popq	%r13
	.cfi_def_cfa_offset 16
	popq	%r14
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE5489:
	.size	assignImagei16, .-assignImagei16
	.p2align 4,,15
	.globl	imageTranspose
	.type	imageTranspose, @function
imageTranspose:
.LFB5467:
	.cfi_startproc
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movq	%rdi, %rax
	movq	%rdi, %r8
	movl	$272, %r11d
	xorl	%r10d, %r10d
	movl	$1, %esi
	movl	$268, %r15d
	subq	%rdi, %r15
	movl	$4, %edx
	subq	%rdi, %rdx
	movq	%rdx, -8(%rsp)
	jmp	.L42
	.p2align 4,,10
	.p2align 3
.L87:
	cmpl	$6, %r10d
	jbe	.L50
	movl	%esi, %edx
	shrl	$3, %edx
	vmovups	268(%r8), %ymm0
	vmovss	1612(%rax), %xmm2
	vinsertps	$0x10, 1880(%rax), %xmm2, %xmm3
	vmovss	1076(%rax), %xmm2
	vinsertps	$0x10, 1344(%rax), %xmm2, %xmm2
	vmovss	540(%rax), %xmm1
	vinsertps	$0x10, 808(%rax), %xmm1, %xmm4
	vmovss	4(%rax), %xmm1
	vinsertps	$0x10, 272(%rax), %xmm1, %xmm1
	vmovlhps	%xmm4, %xmm1, %xmm1
	vmovlhps	%xmm3, %xmm2, %xmm2
	vinsertf128	$0x1, %xmm2, %ymm1, %ymm1
	vmovups	%ymm1, 268(%r8)
	vmovss	%xmm0, 4(%rax)
	vextractps	$1, %xmm0, 272(%rax)
	vextractps	$2, %xmm0, 540(%rax)
	vextractps	$3, %xmm0, 808(%rax)
	vextractf128	$0x1, %ymm0, %xmm0
	vmovss	%xmm0, 1076(%rax)
	vextractps	$1, %xmm0, 1344(%rax)
	vextractps	$2, %xmm0, 1612(%rax)
	vextractps	$3, %xmm0, 1880(%rax)
	cmpl	$1, %edx
	je	.L43
	vmovups	300(%r8), %ymm0
	vmovss	3756(%rax), %xmm2
	vinsertps	$0x10, 4024(%rax), %xmm2, %xmm3
	vmovss	3220(%rax), %xmm2
	vinsertps	$0x10, 3488(%rax), %xmm2, %xmm2
	vmovss	2684(%rax), %xmm1
	vinsertps	$0x10, 2952(%rax), %xmm1, %xmm4
	vmovss	2148(%rax), %xmm1
	vinsertps	$0x10, 2416(%rax), %xmm1, %xmm1
	vmovlhps	%xmm4, %xmm1, %xmm1
	vmovlhps	%xmm3, %xmm2, %xmm2
	vinsertf128	$0x1, %xmm2, %ymm1, %ymm1
	vmovups	%ymm1, 300(%r8)
	vmovss	%xmm0, 2148(%rax)
	vextractps	$1, %xmm0, 2416(%rax)
	vextractps	$2, %xmm0, 2684(%rax)
	vextractps	$3, %xmm0, 2952(%rax)
	vextractf128	$0x1, %ymm0, %xmm0
	vmovss	%xmm0, 3220(%rax)
	vextractps	$1, %xmm0, 3488(%rax)
	vextractps	$2, %xmm0, 3756(%rax)
	vextractps	$3, %xmm0, 4024(%rax)
	cmpl	$2, %edx
	je	.L43
	vmovups	332(%r8), %ymm0
	vmovss	5900(%rax), %xmm2
	vinsertps	$0x10, 6168(%rax), %xmm2, %xmm3
	vmovss	5364(%rax), %xmm2
	vinsertps	$0x10, 5632(%rax), %xmm2, %xmm2
	vmovss	4828(%rax), %xmm1
	vinsertps	$0x10, 5096(%rax), %xmm1, %xmm4
	vmovss	4292(%rax), %xmm1
	vinsertps	$0x10, 4560(%rax), %xmm1, %xmm1
	vmovlhps	%xmm4, %xmm1, %xmm1
	vmovlhps	%xmm3, %xmm2, %xmm2
	vinsertf128	$0x1, %xmm2, %ymm1, %ymm1
	vmovups	%ymm1, 332(%r8)
	vmovss	%xmm0, 4292(%rax)
	vextractps	$1, %xmm0, 4560(%rax)
	vextractps	$2, %xmm0, 4828(%rax)
	vextractps	$3, %xmm0, 5096(%rax)
	vextractf128	$0x1, %ymm0, %xmm0
	vmovss	%xmm0, 5364(%rax)
	vextractps	$1, %xmm0, 5632(%rax)
	vextractps	$2, %xmm0, 5900(%rax)
	vextractps	$3, %xmm0, 6168(%rax)
	cmpl	$3, %edx
	je	.L43
	vmovups	364(%r8), %ymm0
	vmovss	8044(%rax), %xmm2
	vinsertps	$0x10, 8312(%rax), %xmm2, %xmm3
	vmovss	7508(%rax), %xmm2
	vinsertps	$0x10, 7776(%rax), %xmm2, %xmm2
	vmovss	6972(%rax), %xmm1
	vinsertps	$0x10, 7240(%rax), %xmm1, %xmm4
	vmovss	6436(%rax), %xmm1
	vinsertps	$0x10, 6704(%rax), %xmm1, %xmm1
	vmovlhps	%xmm4, %xmm1, %xmm1
	vmovlhps	%xmm3, %xmm2, %xmm2
	vinsertf128	$0x1, %xmm2, %ymm1, %ymm1
	vmovups	%ymm1, 364(%r8)
	vmovss	%xmm0, 6436(%rax)
	vextractps	$1, %xmm0, 6704(%rax)
	vextractps	$2, %xmm0, 6972(%rax)
	vextractps	$3, %xmm0, 7240(%rax)
	vextractf128	$0x1, %ymm0, %xmm0
	vmovss	%xmm0, 7508(%rax)
	vextractps	$1, %xmm0, 7776(%rax)
	vextractps	$2, %xmm0, 8044(%rax)
	vextractps	$3, %xmm0, 8312(%rax)
	cmpl	$4, %edx
	je	.L43
	vmovups	396(%r8), %ymm0
	vmovss	10188(%rax), %xmm2
	vinsertps	$0x10, 10456(%rax), %xmm2, %xmm3
	vmovss	9652(%rax), %xmm2
	vinsertps	$0x10, 9920(%rax), %xmm2, %xmm2
	vmovss	9116(%rax), %xmm1
	vinsertps	$0x10, 9384(%rax), %xmm1, %xmm4
	vmovss	8580(%rax), %xmm1
	vinsertps	$0x10, 8848(%rax), %xmm1, %xmm1
	vmovlhps	%xmm4, %xmm1, %xmm1
	vmovlhps	%xmm3, %xmm2, %xmm2
	vinsertf128	$0x1, %xmm2, %ymm1, %ymm1
	vmovups	%ymm1, 396(%r8)
	vmovss	%xmm0, 8580(%rax)
	vextractps	$1, %xmm0, 8848(%rax)
	vextractps	$2, %xmm0, 9116(%rax)
	vextractps	$3, %xmm0, 9384(%rax)
	vextractf128	$0x1, %ymm0, %xmm0
	vmovss	%xmm0, 9652(%rax)
	vextractps	$1, %xmm0, 9920(%rax)
	vextractps	$2, %xmm0, 10188(%rax)
	vextractps	$3, %xmm0, 10456(%rax)
	cmpl	$5, %edx
	je	.L43
	vmovups	428(%r8), %ymm0
	vmovss	12332(%rax), %xmm2
	vinsertps	$0x10, 12600(%rax), %xmm2, %xmm3
	vmovss	11796(%rax), %xmm2
	vinsertps	$0x10, 12064(%rax), %xmm2, %xmm2
	vmovss	11260(%rax), %xmm1
	vinsertps	$0x10, 11528(%rax), %xmm1, %xmm4
	vmovss	10724(%rax), %xmm1
	vinsertps	$0x10, 10992(%rax), %xmm1, %xmm1
	vmovlhps	%xmm4, %xmm1, %xmm1
	vmovlhps	%xmm3, %xmm2, %xmm2
	vinsertf128	$0x1, %xmm2, %ymm1, %ymm1
	vmovups	%ymm1, 428(%r8)
	vmovss	%xmm0, 10724(%rax)
	vextractps	$1, %xmm0, 10992(%rax)
	vextractps	$2, %xmm0, 11260(%rax)
	vextractps	$3, %xmm0, 11528(%rax)
	vextractf128	$0x1, %ymm0, %xmm0
	vmovss	%xmm0, 11796(%rax)
	vextractps	$1, %xmm0, 12064(%rax)
	vextractps	$2, %xmm0, 12332(%rax)
	vextractps	$3, %xmm0, 12600(%rax)
	cmpl	$7, %edx
	jne	.L43
	vmovups	460(%r8), %ymm0
	vmovss	14476(%rax), %xmm2
	vinsertps	$0x10, 14744(%rax), %xmm2, %xmm3
	vmovss	13940(%rax), %xmm2
	vinsertps	$0x10, 14208(%rax), %xmm2, %xmm2
	vmovss	13404(%rax), %xmm1
	vinsertps	$0x10, 13672(%rax), %xmm1, %xmm4
	vmovss	12868(%rax), %xmm1
	vinsertps	$0x10, 13136(%rax), %xmm1, %xmm1
	vmovlhps	%xmm4, %xmm1, %xmm1
	vmovlhps	%xmm3, %xmm2, %xmm2
	vinsertf128	$0x1, %xmm2, %ymm1, %ymm1
	vmovups	%ymm1, 460(%r8)
	vmovss	%xmm0, 12868(%rax)
	vextractps	$1, %xmm0, 13136(%rax)
	vextractps	$2, %xmm0, 13404(%rax)
	vextractps	$3, %xmm0, 13672(%rax)
	vextractf128	$0x1, %ymm0, %xmm0
	vmovss	%xmm0, 13940(%rax)
	vextractps	$1, %xmm0, 14208(%rax)
	vextractps	$2, %xmm0, 14476(%rax)
	vextractps	$3, %xmm0, 14744(%rax)
.L43:
	movl	%esi, %edx
	andl	$-8, %edx
	cmpl	%esi, %edx
	je	.L86
	movl	%edx, %r8d
	leaq	(%rcx,%r8,4), %r12
	vmovss	(%r12), %xmm0
	imulq	$268, %r8, %r8
	movslq	%esi, %r9
	salq	$2, %r9
	leaq	(%r8,%r9), %rbx
	addq	%rdi, %rbx
	vmovss	(%rbx), %xmm1
	vmovss	%xmm1, (%r12)
	vmovss	%xmm0, (%rbx)
	leal	1(%rdx), %r13d
	cmpl	%r10d, %edx
	jge	.L86
	movslq	%r13d, %rbx
	leaq	(%rcx,%rbx,4), %r14
	vmovss	(%r14), %xmm0
	leaq	268(%r8,%r9), %rbx
	addq	%rdi, %rbx
	vmovss	(%rbx), %xmm1
	vmovss	%xmm1, (%r14)
	vmovss	%xmm0, (%rbx)
	leal	2(%rdx), %r14d
	cmpl	%r10d, %r13d
	jge	.L86
	movslq	%r14d, %rbx
	leaq	(%rcx,%rbx,4), %r13
	vmovss	0(%r13), %xmm0
	leaq	536(%r8,%r9), %rbx
	addq	%rdi, %rbx
	vmovss	(%rbx), %xmm1
	vmovss	%xmm1, 0(%r13)
	vmovss	%xmm0, (%rbx)
	leal	3(%rdx), %r13d
	cmpl	%r10d, %r14d
	jge	.L86
	movslq	%r13d, %rbx
	leaq	(%rcx,%rbx,4), %r14
	vmovss	(%r14), %xmm0
	leaq	804(%r8,%r9), %rbx
	addq	%rdi, %rbx
	vmovss	(%rbx), %xmm1
	vmovss	%xmm1, (%r14)
	vmovss	%xmm0, (%rbx)
	leal	4(%rdx), %r14d
	cmpl	%r10d, %r13d
	jge	.L86
	movslq	%r14d, %rbx
	leaq	(%rcx,%rbx,4), %r13
	vmovss	0(%r13), %xmm0
	leaq	1072(%r8,%r9), %rbx
	addq	%rdi, %rbx
	vmovss	(%rbx), %xmm1
	vmovss	%xmm1, 0(%r13)
	vmovss	%xmm0, (%rbx)
	leal	5(%rdx), %ebx
	cmpl	%r10d, %r14d
	jge	.L86
	movslq	%ebx, %rbx
	movq	%rbx, %r14
	leaq	(%rcx,%rbx,4), %r13
	vmovss	0(%r13), %xmm0
	leaq	1340(%r8,%r9), %rbx
	addq	%rdi, %rbx
	vmovss	(%rbx), %xmm1
	vmovss	%xmm1, 0(%r13)
	vmovss	%xmm0, (%rbx)
	addl	$6, %edx
	cmpl	%r10d, %r14d
	jge	.L86
	movslq	%edx, %rdx
	leaq	(%rcx,%rdx,4), %rbx
	vmovss	(%rbx), %xmm0
	leaq	1608(%r8,%r9), %rdx
	addq	%rdi, %rdx
	vmovss	(%rdx), %xmm1
	vmovss	%xmm1, (%rbx)
	vmovss	%xmm0, (%rdx)
.L86:
	addq	$4, %rax
.L46:
	incl	%esi
	incq	%r10
	movq	%rcx, %r8
	addq	$272, %r11
	cmpl	$64, %esi
	je	.L84
.L42:
	leaq	268(%r8), %rcx
	movq	%rcx, %rdx
	leaq	-264(%r11), %rbx
	leaq	(%r15,%r8), %r9
	cmpq	%r9, %rbx
	setle	%bl
	movq	-8(%rsp), %r14
	leaq	(%r14,%rax), %r9
	cmpq	%r9, %r11
	setle	%r9b
	orb	%r9b, %bl
	jne	.L87
.L50:
	addq	$4, %rax
	movq	%rax, %r8
	leaq	(%rdi,%r11), %r9
	.p2align 4,,10
	.p2align 3
.L45:
	vmovss	(%rdx), %xmm0
	vmovss	(%r8), %xmm1
	vmovss	%xmm1, (%rdx)
	vmovss	%xmm0, (%r8)
	addq	$4, %rdx
	addq	$268, %r8
	cmpq	%r9, %rdx
	jne	.L45
	jmp	.L46
.L84:
	vzeroupper
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE5467:
	.size	imageTranspose, .-imageTranspose
	.p2align 4,,15
	.globl	assignMatrixui16
	.type	assignMatrixui16, @function
assignMatrixui16:
.LFB5468:
	.cfi_startproc
	xorl	%eax, %eax
	vmovdqa	.LC14(%rip), %ymm10
	vmovdqa	.LC15(%rip), %ymm9
	vmovdqa	.LC16(%rip), %ymm1
	vmovdqa	.LC17(%rip), %ymm8
	vmovdqa	.LC18(%rip), %ymm7
	vmovdqa	.LC19(%rip), %ymm6
	vmovdqa	.LC20(%rip), %ymm5
	vmovdqa	.LC21(%rip), %ymm4
	vmovdqa	.LC22(%rip), %ymm3
	.p2align 4,,10
	.p2align 3
.L89:
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpaddd	%ymm10, %ymm0, %ymm2
	vpaddd	%ymm9, %ymm0, %ymm11
	vpand	%ymm2, %ymm1, %ymm2
	vpand	%ymm11, %ymm1, %ymm11
	vpackusdw	%ymm11, %ymm2, %ymm2
	vpermq	$216, %ymm2, %ymm2
	vmovdqu	%ymm2, (%rdi)
	vpaddd	%ymm8, %ymm0, %ymm2
	vpaddd	%ymm7, %ymm0, %ymm11
	vpand	%ymm2, %ymm1, %ymm2
	vpand	%ymm11, %ymm1, %ymm11
	vpackusdw	%ymm11, %ymm2, %ymm2
	vpermq	$216, %ymm2, %ymm2
	vmovdqu	%ymm2, 32(%rdi)
	vpaddd	%ymm6, %ymm0, %ymm2
	vpaddd	%ymm5, %ymm0, %ymm11
	vpand	%ymm2, %ymm1, %ymm2
	vpand	%ymm11, %ymm1, %ymm11
	vpackusdw	%ymm11, %ymm2, %ymm2
	vpermq	$216, %ymm2, %ymm2
	vmovdqu	%ymm2, 64(%rdi)
	vpaddd	%ymm4, %ymm0, %ymm2
	vpaddd	%ymm3, %ymm0, %ymm0
	vpand	%ymm2, %ymm1, %ymm2
	vpand	%ymm0, %ymm1, %ymm0
	vpackusdw	%ymm0, %ymm2, %ymm0
	vpermq	$216, %ymm0, %ymm0
	vmovdqu	%ymm0, 96(%rdi)
	incl	%eax
	subq	$-128, %rdi
	cmpl	$64, %eax
	jne	.L89
	vzeroupper
	ret
	.cfi_endproc
.LFE5468:
	.size	assignMatrixui16, .-assignMatrixui16
	.p2align 4,,15
	.globl	assignMatrixi8
	.type	assignMatrixi8, @function
assignMatrixi8:
.LFB5469:
	.cfi_startproc
	pushq	%r13
	.cfi_def_cfa_offset 16
	.cfi_offset 13, -16
	pushq	%r12
	.cfi_def_cfa_offset 24
	.cfi_offset 12, -24
	pushq	%rbp
	.cfi_def_cfa_offset 32
	.cfi_offset 6, -32
	pushq	%rbx
	.cfi_def_cfa_offset 40
	.cfi_offset 3, -40
	subq	$8, %rsp
	.cfi_def_cfa_offset 48
	leaq	64(%rdi), %rbp
	xorl	%ebx, %ebx
	.p2align 4,,10
	.p2align 3
.L92:
	leaq	-64(%rbp), %r12
	xorl	%r13d, %r13d
	.p2align 4,,10
	.p2align 3
.L93:
	call	rand
	movzbl	%r13b, %edx
	movl	%eax, %ecx
	shrl	$31, %ecx
	addl	%ecx, %eax
	andl	$1, %eax
	subl	%ecx, %eax
	addl	%edx, %eax
	cltd
	shrl	$24, %edx
	addl	%edx, %eax
	movzbl	%al, %eax
	subl	%edx, %eax
	movb	%al, (%r12)
	addl	%ebx, %r13d
	incq	%r12
	cmpq	%rbp, %r12
	jne	.L93
	incl	%ebx
	leaq	64(%r12), %rbp
	cmpl	$64, %ebx
	jne	.L92
	addq	$8, %rsp
	.cfi_def_cfa_offset 40
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%rbp
	.cfi_def_cfa_offset 24
	popq	%r12
	.cfi_def_cfa_offset 16
	popq	%r13
	.cfi_def_cfa_offset 8
	ret
	.cfi_endproc
.LFE5469:
	.size	assignMatrixi8, .-assignMatrixi8
	.p2align 4,,15
	.globl	assignArrayi32
	.type	assignArrayi32, @function
assignArrayi32:
.LFB5470:
	.cfi_startproc
	leaq	16384(%rdi), %rax
	vmovdqa	.LC23(%rip), %ymm4
	vmovdqa	.LC3(%rip), %ymm3
	vmovdqa	.LC24(%rip), %ymm6
	vmovdqa	.LC4(%rip), %ymm5
	vmovdqa	.LC5(%rip), %ymm8
	vmovdqa	.LC6(%rip), %ymm7
	.p2align 4,,10
	.p2align 3
.L99:
	vpmulld	%ymm3, %ymm4, %ymm2
	vpsrlq	$32, %ymm2, %ymm1
	vpmuldq	%ymm5, %ymm2, %ymm0
	vpmuldq	%ymm5, %ymm1, %ymm1
	vpshufb	%ymm8, %ymm0, %ymm0
	vpshufb	%ymm7, %ymm1, %ymm1
	vpor	%ymm1, %ymm0, %ymm0
	vpsrad	$6, %ymm0, %ymm1
	vpslld	$5, %ymm1, %ymm0
	vpsubd	%ymm1, %ymm0, %ymm0
	vpslld	$2, %ymm0, %ymm0
	vpaddd	%ymm1, %ymm0, %ymm0
	vpslld	$3, %ymm0, %ymm0
	vpsubd	%ymm0, %ymm2, %ymm0
	vmovdqu	%ymm0, (%rdi)
	addq	$32, %rdi
	vpaddd	%ymm6, %ymm3, %ymm3
	vpaddd	%ymm6, %ymm4, %ymm4
	cmpq	%rdi, %rax
	jne	.L99
	vzeroupper
	ret
	.cfi_endproc
.LFE5470:
	.size	assignArrayi32, .-assignArrayi32
	.section	.rodata.str1.1,"aMS",@progbits,1
.LC25:
	.string	"a"
.LC26:
	.string	"fileForOutPuts"
.LC27:
	.string	"%s - %dx%d \n"
.LC28:
	.string	"\n\n"
.LC29:
	.string	" A[%d][%d] = %lf, \n"
	.section	.rodata.str1.8,"aMS",@progbits,1
	.align 8
.LC30:
	.string	" \n*************************\n*********************FINISHED*********************\n***************** \n"
	.text
	.p2align 4,,15
	.globl	savefloatMatrixFileForOutPuts
	.type	savefloatMatrixFileForOutPuts, @function
savefloatMatrixFileForOutPuts:
.LFB5483:
	.cfi_startproc
	pushq	%r12
	.cfi_def_cfa_offset 16
	.cfi_offset 12, -16
	pushq	%rbp
	.cfi_def_cfa_offset 24
	.cfi_offset 6, -24
	pushq	%rbx
	.cfi_def_cfa_offset 32
	.cfi_offset 3, -32
	movq	%rdi, %r12
	movl	$.LC25, %esi
	movl	$.LC26, %edi
	call	fopen
	movq	%rax, fileForOutPuts(%rip)
	movl	$64, %r8d
	movl	$64, %ecx
	movq	programName(%rip), %rdx
	movl	$.LC27, %esi
	movq	%rax, %rdi
	xorl	%eax, %eax
	call	fprintf
	xorl	%ebp, %ebp
	.p2align 4,,10
	.p2align 3
.L103:
	movq	fileForOutPuts(%rip), %rcx
	movl	$2, %edx
	movl	$1, %esi
	movl	$.LC28, %edi
	call	fwrite
	xorl	%ebx, %ebx
	.p2align 4,,10
	.p2align 3
.L102:
	vcvtss2sd	(%r12,%rbx,4), %xmm0, %xmm0
	movl	%ebx, %ecx
	movl	%ebp, %edx
	movl	$.LC29, %esi
	movq	fileForOutPuts(%rip), %rdi
	movb	$1, %al
	call	fprintf
	incq	%rbx
	cmpq	$64, %rbx
	jne	.L102
	incl	%ebp
	addq	$256, %r12
	cmpl	$64, %ebp
	jne	.L103
	movq	fileForOutPuts(%rip), %rcx
	movl	$98, %edx
	movl	$1, %esi
	movl	$.LC30, %edi
	call	fwrite
	movq	fileForOutPuts(%rip), %rdi
	popq	%rbx
	.cfi_def_cfa_offset 24
	popq	%rbp
	.cfi_def_cfa_offset 16
	popq	%r12
	.cfi_def_cfa_offset 8
	jmp	fclose
	.cfi_endproc
.LFE5483:
	.size	savefloatMatrixFileForOutPuts, .-savefloatMatrixFileForOutPuts
	.p2align 4,,15
	.globl	savefloatMatrixFileName
	.type	savefloatMatrixFileName, @function
savefloatMatrixFileName:
.LFB5484:
	.cfi_startproc
	pushq	%r12
	.cfi_def_cfa_offset 16
	.cfi_offset 12, -16
	pushq	%rbp
	.cfi_def_cfa_offset 24
	.cfi_offset 6, -24
	pushq	%rbx
	.cfi_def_cfa_offset 32
	.cfi_offset 3, -32
	movq	%rdi, %r12
	movq	%rsi, %rdi
	movl	$.LC25, %esi
	call	fopen
	movq	%rax, fileForOutPuts(%rip)
	movl	$64, %r8d
	movl	$64, %ecx
	movq	programName(%rip), %rdx
	movl	$.LC27, %esi
	movq	%rax, %rdi
	xorl	%eax, %eax
	call	fprintf
	xorl	%ebp, %ebp
	.p2align 4,,10
	.p2align 3
.L109:
	movq	fileForOutPuts(%rip), %rcx
	movl	$2, %edx
	movl	$1, %esi
	movl	$.LC28, %edi
	call	fwrite
	xorl	%ebx, %ebx
	.p2align 4,,10
	.p2align 3
.L108:
	vcvtss2sd	(%r12,%rbx,4), %xmm0, %xmm0
	movl	%ebx, %ecx
	movl	%ebp, %edx
	movl	$.LC29, %esi
	movq	fileForOutPuts(%rip), %rdi
	movb	$1, %al
	call	fprintf
	incq	%rbx
	cmpq	$64, %rbx
	jne	.L108
	incl	%ebp
	addq	$256, %r12
	cmpl	$64, %ebp
	jne	.L109
	movq	fileForOutPuts(%rip), %rcx
	movl	$98, %edx
	movl	$1, %esi
	movl	$.LC30, %edi
	call	fwrite
	movq	fileForOutPuts(%rip), %rdi
	popq	%rbx
	.cfi_def_cfa_offset 24
	popq	%rbp
	.cfi_def_cfa_offset 16
	popq	%r12
	.cfi_def_cfa_offset 8
	jmp	fclose
	.cfi_endproc
.LFE5484:
	.size	savefloatMatrixFileName, .-savefloatMatrixFileName
	.section	.rodata.str1.1
.LC31:
	.string	" A[%d][%d] = %d, \n"
	.text
	.p2align 4,,15
	.globl	saveintMatrixFileForOutPuts
	.type	saveintMatrixFileForOutPuts, @function
saveintMatrixFileForOutPuts:
.LFB5485:
	.cfi_startproc
	pushq	%r12
	.cfi_def_cfa_offset 16
	.cfi_offset 12, -16
	pushq	%rbp
	.cfi_def_cfa_offset 24
	.cfi_offset 6, -24
	pushq	%rbx
	.cfi_def_cfa_offset 32
	.cfi_offset 3, -32
	movq	%rdi, %r12
	movl	$.LC25, %esi
	movl	$.LC26, %edi
	call	fopen
	movq	%rax, fileForOutPuts(%rip)
	movl	$64, %r8d
	movl	$64, %ecx
	movq	programName(%rip), %rdx
	movl	$.LC27, %esi
	movq	%rax, %rdi
	xorl	%eax, %eax
	call	fprintf
	xorl	%ebp, %ebp
	.p2align 4,,10
	.p2align 3
.L115:
	movq	fileForOutPuts(%rip), %rcx
	movl	$2, %edx
	movl	$1, %esi
	movl	$.LC28, %edi
	call	fwrite
	xorl	%ebx, %ebx
	.p2align 4,,10
	.p2align 3
.L114:
	movl	(%r12,%rbx,4), %r8d
	movl	%ebx, %ecx
	movl	%ebp, %edx
	movl	$.LC31, %esi
	movq	fileForOutPuts(%rip), %rdi
	xorl	%eax, %eax
	call	fprintf
	incq	%rbx
	cmpq	$64, %rbx
	jne	.L114
	incl	%ebp
	addq	$256, %r12
	cmpl	$64, %ebp
	jne	.L115
	movq	fileForOutPuts(%rip), %rcx
	movl	$98, %edx
	movl	$1, %esi
	movl	$.LC30, %edi
	call	fwrite
	movq	fileForOutPuts(%rip), %rdi
	popq	%rbx
	.cfi_def_cfa_offset 24
	popq	%rbp
	.cfi_def_cfa_offset 16
	popq	%r12
	.cfi_def_cfa_offset 8
	jmp	fclose
	.cfi_endproc
.LFE5485:
	.size	saveintMatrixFileForOutPuts, .-saveintMatrixFileForOutPuts
	.p2align 4,,15
	.globl	saveintMatrixFileName
	.type	saveintMatrixFileName, @function
saveintMatrixFileName:
.LFB5486:
	.cfi_startproc
	pushq	%r12
	.cfi_def_cfa_offset 16
	.cfi_offset 12, -16
	pushq	%rbp
	.cfi_def_cfa_offset 24
	.cfi_offset 6, -24
	pushq	%rbx
	.cfi_def_cfa_offset 32
	.cfi_offset 3, -32
	movq	%rdi, %r12
	movq	%rsi, %rdi
	movl	$.LC25, %esi
	call	fopen
	movq	%rax, fileForOutPuts(%rip)
	movl	$64, %r8d
	movl	$64, %ecx
	movq	programName(%rip), %rdx
	movl	$.LC27, %esi
	movq	%rax, %rdi
	xorl	%eax, %eax
	call	fprintf
	xorl	%ebp, %ebp
	.p2align 4,,10
	.p2align 3
.L121:
	movq	fileForOutPuts(%rip), %rcx
	movl	$2, %edx
	movl	$1, %esi
	movl	$.LC28, %edi
	call	fwrite
	xorl	%ebx, %ebx
	.p2align 4,,10
	.p2align 3
.L120:
	movl	(%r12,%rbx,4), %r8d
	movl	%ebx, %ecx
	movl	%ebp, %edx
	movl	$.LC31, %esi
	movq	fileForOutPuts(%rip), %rdi
	xorl	%eax, %eax
	call	fprintf
	incq	%rbx
	cmpq	$64, %rbx
	jne	.L120
	incl	%ebp
	addq	$256, %r12
	cmpl	$64, %ebp
	jne	.L121
	movq	fileForOutPuts(%rip), %rcx
	movl	$98, %edx
	movl	$1, %esi
	movl	$.LC30, %edi
	call	fwrite
	movq	fileForOutPuts(%rip), %rdi
	popq	%rbx
	.cfi_def_cfa_offset 24
	popq	%rbp
	.cfi_def_cfa_offset 16
	popq	%r12
	.cfi_def_cfa_offset 8
	jmp	fclose
	.cfi_endproc
.LFE5486:
	.size	saveintMatrixFileName, .-saveintMatrixFileName
	.section	.rodata.str1.1
.LC32:
	.string	"L53FULLTRACOLCOL"
.LC33:
	.string	"inputs"
	.section	.rodata.str1.8
	.align 8
.LC34:
	.string	"\nthe best is %lld in %lldth iteration and %lld repetitions\n"
	.section	.rodata.str1.1
.LC35:
	.string	"fileForSpeedups"
.LC36:
	.string	"%s, %dx%d, %lld\n"
.LC37:
	.string	"output = %d\n"
	.section	.text.startup,"ax",@progbits
	.p2align 4,,15
	.globl	main
	.type	main, @function
main:
.LFB5487:
	.cfi_startproc
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset 6, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register 6
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	subq	$64, %rsp
	.cfi_offset 15, -24
	.cfi_offset 14, -32
	.cfi_offset 13, -40
	.cfi_offset 12, -48
	.cfi_offset 3, -56
	movl	$3, %esi
	movl	$2, %edi
	call	assignToThisCore12
	movq	$.LC32, programName(%rip)
	movl	$32, half_row(%rip)
	movl	$32, half_col(%rip)
	movl	$in_image, %r9d
	xorl	%edi, %edi
	movl	$-2139062143, %r10d
.L126:
	xorl	%r8d, %r8d
	movl	$1, %esi
	.p2align 4,,10
	.p2align 3
.L127:
	movl	%edi, %eax
	cltd
	idivl	%esi
	leal	(%rax,%r8), %ecx
	movl	%ecx, %eax
	mull	%r10d
	shrl	$7, %edx
	movl	%edx, %eax
	sall	$8, %eax
	subl	%edx, %eax
	subl	%eax, %ecx
	movl	%ecx, -4(%r9,%rsi,4)
	incq	%rsi
	addl	%edi, %r8d
	cmpq	$65, %rsi
	jne	.L127
	incl	%edi
	addq	$272, %r9
	cmpl	$64, %edi
	jne	.L126
	movl	$.LC33, %esi
	movl	$in_image, %edi
	call	saveintMatrixFileName
	movq	$0, elapsed_rdtsc(%rip)
	movabsq	$9999999999, %rax
	movq	%rax, overal_time(%rip)
	movq	$0, ttime(%rip)
#APP
# 25 "IMP1.c" 1
	#mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm1
# 0 "" 2
#NO_APP
	rdtsc
	salq	$32, %rdx
	orq	%rdx, %rax
	movq	%rax, t1_rdtsc(%rip)
	movl	$272, %eax
	subq	$in_image, %rax
	movq	%rax, %r13
.L129:
	movl	$in_image+276, %r11d
	movl	$in_image, %eax
	movq	%rax, %rdx
	movl	$276, %ebx
	movl	$1, %edi
	xorl	%r8d, %r8d
	movq	%r11, 56(%rsp)
	jmp	.L130
	.p2align 4,,10
	.p2align 3
.L197:
	cmpl	$6, %r8d
	jbe	.L138
	movl	%r9d, %ecx
	shrl	$3, %ecx
	vcvtdq2ps	272(%rdx), %ymm0
	vmovd	1636(%rax), %xmm5
	vpinsrd	$1, 1908(%rax), %xmm5, %xmm3
	vmovd	1092(%rax), %xmm6
	vpinsrd	$1, 1364(%rax), %xmm6, %xmm2
	vmovd	548(%rax), %xmm7
	vpinsrd	$1, 820(%rax), %xmm7, %xmm4
	vmovd	4(%rax), %xmm5
	vpinsrd	$1, 276(%rax), %xmm5, %xmm1
	vpunpcklqdq	%xmm4, %xmm1, %xmm1
	vpunpcklqdq	%xmm3, %xmm2, %xmm2
	vinserti128	$0x1, %xmm2, %ymm1, %ymm1
	vmovdqu	%ymm1, 272(%rdx)
	vcvttps2dq	%ymm0, %ymm0
	vmovd	%xmm0, 4(%rax)
	vpextrd	$1, %xmm0, 276(%rax)
	vpextrd	$2, %xmm0, 548(%rax)
	vpextrd	$3, %xmm0, 820(%rax)
	vextracti128	$0x1, %ymm0, %xmm0
	vmovd	%xmm0, 1092(%rax)
	vpextrd	$1, %xmm0, 1364(%rax)
	vpextrd	$2, %xmm0, 1636(%rax)
	vpextrd	$3, %xmm0, 1908(%rax)
	cmpl	$1, %ecx
	je	.L131
	vcvtdq2ps	304(%rdx), %ymm0
	vmovd	3812(%rax), %xmm6
	vpinsrd	$1, 4084(%rax), %xmm6, %xmm3
	vmovd	3268(%rax), %xmm7
	vpinsrd	$1, 3540(%rax), %xmm7, %xmm2
	vmovd	2724(%rax), %xmm6
	vpinsrd	$1, 2996(%rax), %xmm6, %xmm4
	vmovd	2180(%rax), %xmm7
	vpinsrd	$1, 2452(%rax), %xmm7, %xmm1
	vpunpcklqdq	%xmm4, %xmm1, %xmm1
	vpunpcklqdq	%xmm3, %xmm2, %xmm2
	vinserti128	$0x1, %xmm2, %ymm1, %ymm1
	vmovdqu	%ymm1, 304(%rdx)
	vcvttps2dq	%ymm0, %ymm0
	vmovd	%xmm0, 2180(%rax)
	vpextrd	$1, %xmm0, 2452(%rax)
	vpextrd	$2, %xmm0, 2724(%rax)
	vpextrd	$3, %xmm0, 2996(%rax)
	vextracti128	$0x1, %ymm0, %xmm0
	vmovd	%xmm0, 3268(%rax)
	vpextrd	$1, %xmm0, 3540(%rax)
	vpextrd	$2, %xmm0, 3812(%rax)
	vpextrd	$3, %xmm0, 4084(%rax)
	cmpl	$2, %ecx
	je	.L131
	vcvtdq2ps	336(%rdx), %ymm0
	vmovd	5988(%rax), %xmm5
	vpinsrd	$1, 6260(%rax), %xmm5, %xmm3
	vmovd	5444(%rax), %xmm5
	vpinsrd	$1, 5716(%rax), %xmm5, %xmm2
	vmovd	4900(%rax), %xmm6
	vpinsrd	$1, 5172(%rax), %xmm6, %xmm4
	vmovd	4356(%rax), %xmm7
	vpinsrd	$1, 4628(%rax), %xmm7, %xmm1
	vpunpcklqdq	%xmm4, %xmm1, %xmm1
	vpunpcklqdq	%xmm3, %xmm2, %xmm2
	vinserti128	$0x1, %xmm2, %ymm1, %ymm1
	vmovdqu	%ymm1, 336(%rdx)
	vcvttps2dq	%ymm0, %ymm0
	vmovd	%xmm0, 4356(%rax)
	vpextrd	$1, %xmm0, 4628(%rax)
	vpextrd	$2, %xmm0, 4900(%rax)
	vpextrd	$3, %xmm0, 5172(%rax)
	vextracti128	$0x1, %ymm0, %xmm0
	vmovd	%xmm0, 5444(%rax)
	vpextrd	$1, %xmm0, 5716(%rax)
	vpextrd	$2, %xmm0, 5988(%rax)
	vpextrd	$3, %xmm0, 6260(%rax)
	cmpl	$3, %ecx
	je	.L131
	vcvtdq2ps	368(%rdx), %ymm0
	vmovd	8164(%rax), %xmm5
	vpinsrd	$1, 8436(%rax), %xmm5, %xmm3
	vmovd	7620(%rax), %xmm6
	vpinsrd	$1, 7892(%rax), %xmm6, %xmm2
	vmovd	7076(%rax), %xmm7
	vpinsrd	$1, 7348(%rax), %xmm7, %xmm4
	vmovd	6532(%rax), %xmm5
	vpinsrd	$1, 6804(%rax), %xmm5, %xmm1
	vpunpcklqdq	%xmm4, %xmm1, %xmm1
	vpunpcklqdq	%xmm3, %xmm2, %xmm2
	vinserti128	$0x1, %xmm2, %ymm1, %ymm1
	vmovdqu	%ymm1, 368(%rdx)
	vcvttps2dq	%ymm0, %ymm0
	vmovd	%xmm0, 6532(%rax)
	vpextrd	$1, %xmm0, 6804(%rax)
	vpextrd	$2, %xmm0, 7076(%rax)
	vpextrd	$3, %xmm0, 7348(%rax)
	vextracti128	$0x1, %ymm0, %xmm0
	vmovd	%xmm0, 7620(%rax)
	vpextrd	$1, %xmm0, 7892(%rax)
	vpextrd	$2, %xmm0, 8164(%rax)
	vpextrd	$3, %xmm0, 8436(%rax)
	cmpl	$4, %ecx
	je	.L131
	vcvtdq2ps	400(%rdx), %ymm0
	vmovd	10340(%rax), %xmm6
	vpinsrd	$1, 10612(%rax), %xmm6, %xmm3
	vmovd	9796(%rax), %xmm7
	vpinsrd	$1, 10068(%rax), %xmm7, %xmm2
	vmovd	9252(%rax), %xmm4
	vpinsrd	$1, 9524(%rax), %xmm4, %xmm4
	vmovd	8708(%rax), %xmm6
	vpinsrd	$1, 8980(%rax), %xmm6, %xmm1
	vpunpcklqdq	%xmm4, %xmm1, %xmm1
	vpunpcklqdq	%xmm3, %xmm2, %xmm2
	vinserti128	$0x1, %xmm2, %ymm1, %ymm1
	vmovdqu	%ymm1, 400(%rdx)
	vcvttps2dq	%ymm0, %ymm0
	vmovd	%xmm0, 8708(%rax)
	vpextrd	$1, %xmm0, 8980(%rax)
	vpextrd	$2, %xmm0, 9252(%rax)
	vpextrd	$3, %xmm0, 9524(%rax)
	vextracti128	$0x1, %ymm0, %xmm0
	vmovd	%xmm0, 9796(%rax)
	vpextrd	$1, %xmm0, 10068(%rax)
	vpextrd	$2, %xmm0, 10340(%rax)
	vpextrd	$3, %xmm0, 10612(%rax)
	cmpl	$5, %ecx
	je	.L131
	vcvtdq2ps	432(%rdx), %ymm0
	vmovd	12516(%rax), %xmm3
	vpinsrd	$1, 12788(%rax), %xmm3, %xmm3
	vmovd	11972(%rax), %xmm7
	vpinsrd	$1, 12244(%rax), %xmm7, %xmm2
	vmovd	11428(%rax), %xmm5
	vpinsrd	$1, 11700(%rax), %xmm5, %xmm4
	vmovd	10884(%rax), %xmm7
	vpinsrd	$1, 11156(%rax), %xmm7, %xmm1
	vpunpcklqdq	%xmm4, %xmm1, %xmm1
	vpunpcklqdq	%xmm3, %xmm2, %xmm2
	vinserti128	$0x1, %xmm2, %ymm1, %ymm1
	vmovdqu	%ymm1, 432(%rdx)
	vcvttps2dq	%ymm0, %ymm0
	vmovd	%xmm0, 10884(%rax)
	vpextrd	$1, %xmm0, 11156(%rax)
	vpextrd	$2, %xmm0, 11428(%rax)
	vpextrd	$3, %xmm0, 11700(%rax)
	vextracti128	$0x1, %ymm0, %xmm0
	vmovd	%xmm0, 11972(%rax)
	vpextrd	$1, %xmm0, 12244(%rax)
	vpextrd	$2, %xmm0, 12516(%rax)
	vpextrd	$3, %xmm0, 12788(%rax)
	cmpl	$7, %ecx
	jne	.L131
	vcvtdq2ps	464(%rdx), %ymm0
	vmovd	14692(%rax), %xmm3
	vpinsrd	$1, 14964(%rax), %xmm3, %xmm3
	vmovd	14148(%rax), %xmm4
	vpinsrd	$1, 14420(%rax), %xmm4, %xmm2
	vmovd	13604(%rax), %xmm5
	vpinsrd	$1, 13876(%rax), %xmm5, %xmm4
	vmovd	13060(%rax), %xmm6
	vpinsrd	$1, 13332(%rax), %xmm6, %xmm1
	vpunpcklqdq	%xmm4, %xmm1, %xmm1
	vpunpcklqdq	%xmm3, %xmm2, %xmm2
	vinserti128	$0x1, %xmm2, %ymm1, %ymm1
	vmovdqu	%ymm1, 464(%rdx)
	vcvttps2dq	%ymm0, %ymm0
	vmovd	%xmm0, 13060(%rax)
	vpextrd	$1, %xmm0, 13332(%rax)
	vpextrd	$2, %xmm0, 13604(%rax)
	vpextrd	$3, %xmm0, 13876(%rax)
	vextracti128	$0x1, %ymm0, %xmm0
	vmovd	%xmm0, 14148(%rax)
	vpextrd	$1, %xmm0, 14420(%rax)
	vpextrd	$2, %xmm0, 14692(%rax)
	vpextrd	$3, %xmm0, 14964(%rax)
.L131:
	movl	%r9d, %ecx
	andl	$-8, %ecx
	cmpl	%r9d, %ecx
	je	.L196
	movslq	%ecx, %r10
	imulq	$68, %rsi, %r9
	leaq	(%r9,%r10), %r12
	vcvtsi2ss	in_image(,%r12,4), %xmm0, %xmm0
	imulq	$68, %r10, %r10
	addq	%rsi, %r10
	movl	in_image(,%r10,4), %r14d
	movl	%r14d, in_image(,%r12,4)
	vcvttss2si	%xmm0, %r12d
	movl	%r12d, in_image(,%r10,4)
	leal	1(%rcx), %r14d
	cmpl	%r8d, %ecx
	jge	.L196
	movslq	%r14d, %r10
	leaq	(%r9,%r10), %r15
	vcvtsi2ss	in_image(,%r15,4), %xmm0, %xmm0
	imulq	$68, %r10, %r10
	addq	%rsi, %r10
	movl	in_image(,%r10,4), %r11d
	movl	%r11d, in_image(,%r15,4)
	vcvttss2si	%xmm0, %r15d
	movl	%r15d, in_image(,%r10,4)
	leal	2(%rcx), %r15d
	cmpl	%r8d, %r14d
	jge	.L196
	movslq	%r15d, %r10
	leaq	(%r9,%r10), %r14
	vcvtsi2ss	in_image(,%r14,4), %xmm0, %xmm0
	imulq	$68, %r10, %r10
	addq	%rsi, %r10
	movl	in_image(,%r10,4), %r11d
	movl	%r11d, in_image(,%r14,4)
	vcvttss2si	%xmm0, %r14d
	movl	%r14d, in_image(,%r10,4)
	leal	3(%rcx), %r14d
	cmpl	%r8d, %r15d
	jge	.L196
	movslq	%r14d, %r10
	leaq	(%r9,%r10), %r15
	vcvtsi2ss	in_image(,%r15,4), %xmm0, %xmm0
	imulq	$68, %r10, %r10
	addq	%rsi, %r10
	movl	in_image(,%r10,4), %r11d
	movl	%r11d, in_image(,%r15,4)
	vcvttss2si	%xmm0, %r15d
	movl	%r15d, in_image(,%r10,4)
	leal	4(%rcx), %r15d
	cmpl	%r8d, %r14d
	jge	.L196
	movslq	%r15d, %r10
	leaq	(%r9,%r10), %r14
	vcvtsi2ss	in_image(,%r14,4), %xmm0, %xmm0
	imulq	$68, %r10, %r10
	addq	%rsi, %r10
	movl	in_image(,%r10,4), %r11d
	movl	%r11d, in_image(,%r14,4)
	vcvttss2si	%xmm0, %r14d
	movl	%r14d, in_image(,%r10,4)
	leal	5(%rcx), %r14d
	cmpl	%r8d, %r15d
	jge	.L196
	movslq	%r14d, %r10
	leaq	(%r9,%r10), %r15
	vcvtsi2ss	in_image(,%r15,4), %xmm0, %xmm0
	imulq	$68, %r10, %r10
	addq	%rsi, %r10
	movl	in_image(,%r10,4), %r11d
	movl	%r11d, in_image(,%r15,4)
	vcvttss2si	%xmm0, %r15d
	movl	%r15d, in_image(,%r10,4)
	addl	$6, %ecx
	cmpl	%r8d, %r14d
	jge	.L196
	movslq	%ecx, %rcx
	addq	%rcx, %r9
	vcvtsi2ss	in_image(,%r9,4), %xmm0, %xmm0
	imulq	$68, %rcx, %rcx
	addq	%rcx, %rsi
	movl	in_image(,%rsi,4), %ecx
	movl	%ecx, in_image(,%r9,4)
	vcvttss2si	%xmm0, %ecx
	movl	%ecx, in_image(,%rsi,4)
.L196:
	addq	$272, %rdx
	addq	$4, %rax
.L134:
	incq	%r8
	incq	%rdi
	addq	$276, 56(%rsp)
	addq	$276, %rbx
	cmpq	$64, %rdi
	je	.L135
.L130:
	movl	%edi, %r9d
	movslq	%edi, %rsi
	leaq	0(%r13,%rdx), %r10
	leaq	-268(%rbx), %rcx
	cmpq	%rcx, %r10
	jge	.L197
.L138:
	addq	$272, %rdx
	movq	%rdx, %rsi
	addq	$4, %rax
	movq	%rax, %rcx
	movq	56(%rsp), %r11
	.p2align 4,,10
	.p2align 3
.L133:
	vcvtsi2ss	(%rsi), %xmm0, %xmm0
	movl	(%rcx), %r9d
	movl	%r9d, (%rsi)
	vcvttss2si	%xmm0, %r9d
	movl	%r9d, (%rcx)
	addq	$4, %rsi
	addq	$272, %rcx
	cmpq	%r11, %rcx
	jne	.L133
	movq	%r11, 56(%rsp)
	jmp	.L134
.L135:
	movslq	half_row(%rip), %rax
	movq	%rax, %rdx
	movq	%rax, 40(%rsp)
	imulq	$272, %rax, %r9
	movq	%r9, 48(%rsp)
	movl	$in_image, %ecx
	subq	%r9, %rcx
	imulq	$-272, %rax, %rax
	leal	31(%rdx), %esi
	movslq	%esi, %rsi
	imulq	$272, %rsi, %rbx
	movq	%rbx, 24(%rsp)
	leal	30(%rdx), %esi
	movslq	%esi, %rsi
	imulq	$272, %rsi, %rsi
	leaq	(%rsi,%rax), %rbx
	movq	%rbx, 16(%rsp)
	leal	32(%rdx), %esi
	movslq	%esi, %rsi
	imulq	$272, %rsi, %rbx
	movq	%rbx, 8(%rsp)
	addl	$33, %edx
	movslq	%edx, %rdx
	imulq	$272, %rdx, %rbx
	movq	%rbx, 32(%rsp)
	xorl	%r8d, %r8d
	leaq	272(%rcx), %r15
	.p2align 4,,10
	.p2align 3
.L141:
	movslq	%r8d, %r14
	movl	in_image-272(%r9), %ebx
	movl	in_image(,%r8,4), %edx
	leaq	(%r15,%r9), %rsi
	movq	%r9, %rcx
	xorl	%r10d, %r10d
	movl	$1, %r11d
	movq	%r8, 56(%rsp)
.L139:
	movl	272(%rsi), %r12d
	leal	(%r12,%rdx), %edi
	sarl	%edi
	movl	(%rsi), %r8d
	subl	%edi, %r8d
	movl	%r8d, ou_image(%rcx)
	movl	in_image(%rcx), %edi
	leal	2(%rdi,%rbx), %ebx
	sarl	$2, %ebx
	addl	%ebx, %edx
	movl	%edx, ou_image(%rax,%rcx)
	movl	816(%rsi), %edx
	leal	(%r12,%rdx), %ebx
	sarl	%ebx
	movl	544(%rsi), %r8d
	subl	%ebx, %r8d
	movl	%r8d, ou_image+272(%rcx)
	movl	in_image+272(%rcx), %ebx
	leal	2(%rdi,%rbx), %edi
	sarl	$2, %edi
	addl	%r12d, %edi
	movl	%edi, ou_image+272(%rax,%rcx)
	movl	%r10d, %edi
	addl	$2, %r10d
	addq	$1088, %rsi
	addq	$544, %rcx
	addl	$4, %r11d
	cmpl	$61, %r11d
	jne	.L139
	movq	56(%rsp), %r8
	movl	in_image+16320(,%r8,4), %esi
	movl	in_image+16864(,%r8,4), %ecx
	leal	(%rsi,%rcx), %edx
	sarl	%edx
	movl	in_image+16592(,%r14,4), %ebx
	subl	%edx, %ebx
	movl	%ebx, ou_image+8160(%r9)
	movslq	%r10d, %r11
	imulq	$68, %r11, %r11
	addq	%r14, %r11
	movl	in_image+7888(%r9), %edx
	addl	in_image+8160(%r9), %edx
	addl	$2, %edx
	sarl	$2, %edx
	addl	%esi, %edx
	movl	%edx, ou_image(,%r11,4)
	movl	in_image+17408(,%r8,4), %esi
	leal	(%rcx,%rsi), %edx
	sarl	%edx
	movl	in_image+17136(,%r14,4), %ebx
	subl	%edx, %ebx
	movl	%ebx, %edx
	movq	24(%rsp), %rbx
	movl	%edx, ou_image(%rbx,%r8,4)
	movl	in_image(%rbx,%r8,4), %r11d
	leal	3(%rdi), %edx
	movslq	%edx, %rdx
	imulq	$68, %rdx, %rdx
	addq	%r14, %rdx
	movq	16(%rsp), %r12
	movl	in_image(%r9,%r12), %ebx
	addl	%r11d, %ebx
	addl	$2, %ebx
	sarl	$2, %ebx
	addl	%ebx, %ecx
	movl	%ecx, ou_image(,%rdx,4)
	leal	4(%rdi), %ecx
	movl	$30, %ebx
	cmpl	%r10d, %ebx
	je	.L140
	movl	in_image+17952(,%r8,4), %ebx
	leal	(%rsi,%rbx), %edx
	sarl	%edx
	movl	in_image+17680(,%r14,4), %r12d
	subl	%edx, %r12d
	movl	%r12d, %edx
	movq	8(%rsp), %r12
	movl	%edx, ou_image(%r12,%r8,4)
	movl	in_image(%r12,%r8,4), %edx
	movslq	%ecx, %rcx
	imulq	$68, %rcx, %rcx
	addq	%r14, %rcx
	leal	2(%r11,%rdx), %r11d
	sarl	$2, %r11d
	addl	%r11d, %esi
	movl	%esi, ou_image(,%rcx,4)
	leal	5(%rdi), %ecx
	cmpl	$29, %r10d
	je	.L140
	movl	in_image+18496(,%r8,4), %esi
	addl	%ebx, %esi
	sarl	%esi
	movl	in_image+18224(,%r14,4), %edi
	subl	%esi, %edi
	movl	%edi, %esi
	movq	32(%rsp), %rdi
	movl	%esi, ou_image(%rdi,%r8,4)
	movslq	%ecx, %rcx
	imulq	$68, %rcx, %rcx
	addq	%rcx, %r14
	addl	in_image(%rdi,%r8,4), %edx
	addl	$2, %edx
	sarl	$2, %edx
	addl	%edx, %ebx
	movl	%ebx, ou_image(,%r14,4)
.L140:
	incq	%r8
	addq	$4, %r9
	cmpq	$64, %r8
	jne	.L141
	imulq	$-544, 40(%rsp), %r8
	movq	48(%rsp), %rbx
	leaq	8704(%rbx), %r9
	leaq	8960(%rbx), %r10
	.p2align 4,,10
	.p2align 3
.L143:
	movl	ou_image-8704(%rax,%r9), %r11d
	leaq	-8704(%r9), %rcx
	.p2align 4,,10
	.p2align 3
.L142:
	movl	ou_image+544(%r8,%rcx,2), %edi
	leal	(%rdi,%r11), %esi
	sarl	%esi
	movl	ou_image+272(%r8,%rcx,2), %edx
	subl	%esi, %edx
	movl	%edx, in_image(%rcx)
	addl	in_image-272(%rcx), %edx
	addl	$2, %edx
	sarl	$2, %edx
	addl	%r11d, %edx
	movl	%edx, in_image(%rax,%rcx)
	addq	$272, %rcx
	movl	%edi, %r11d
	cmpq	%r9, %rcx
	jne	.L142
	subq	$4, %r8
	leaq	4(%rcx), %r9
	cmpq	%r10, %r9
	jne	.L143
	movl	$32, jj(%rip)
	rdtsc
	salq	$32, %rdx
	orq	%rdx, %rax
	movq	%rax, t2_rdtsc(%rip)
#APP
# 52 "IMP1.c" 1
	#mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm2
# 0 "" 2
#NO_APP
	movq	t2_rdtsc(%rip), %rax
	subq	t1_rdtsc(%rip), %rax
	movq	%rax, ttotal_rdtsc(%rip)
	movq	ttbest_rdtsc(%rip), %rsi
	movq	elapsed_rdtsc(%rip), %rdx
	cmpq	%rsi, %rax
	jge	.L145
	movq	%rax, ttbest_rdtsc(%rip)
	movq	elapsed_rdtsc(%rip), %rdx
	movq	%rdx, %rcx
	negq	%rcx
	movq	%rcx, elapsed(%rip)
	movq	%rax, %rsi
.L145:
	addq	ttime(%rip), %rax
	movq	%rax, ttime(%rip)
	leaq	-1(%rdx), %rcx
	movq	%rcx, elapsed_rdtsc(%rip)
	testq	%rdx, %rdx
	je	.L146
	cmpq	overal_time(%rip), %rax
	jge	.L146
#APP
# 25 "IMP1.c" 1
	#mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm1
# 0 "" 2
#NO_APP
	rdtsc
	salq	$32, %rdx
	orq	%rdx, %rax
	movq	%rax, t1_rdtsc(%rip)
	jmp	.L129
.L146:
	movl	$1, %ecx
	subq	%rdx, %rcx
	movq	elapsed(%rip), %rdx
	movl	$.LC34, %edi
	xorl	%eax, %eax
	vzeroupper
	call	printf
	movl	$.LC25, %esi
	movl	$.LC35, %edi
	call	fopen
	movq	%rax, fileForSpeedups(%rip)
	movq	ttbest_rdtsc(%rip), %r9
	movl	$64, %r8d
	movl	$64, %ecx
	movq	programName(%rip), %rdx
	movl	$.LC36, %esi
	movq	%rax, %rdi
	xorl	%eax, %eax
	call	fprintf
	movl	in_image+8832(%rip), %esi
	movl	$.LC37, %edi
	xorl	%eax, %eax
	call	printf
	movl	$in_image, %edi
	call	saveintMatrixFileForOutPuts
	xorl	%eax, %eax
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa 7, 8
	ret
	.cfi_endproc
.LFE5487:
	.size	main, .-main
	.comm	ou_image,18496,32
	.comm	in_image,18496,32
	.comm	half_col,4,4
	.comm	half_row,4,4
	.comm	jj,4,4
	.comm	j,4,4
	.comm	i,4,4
	.globl	fileForOutPuts
	.bss
	.align 8
	.type	fileForOutPuts, @object
	.size	fileForOutPuts, 8
fileForOutPuts:
	.zero	8
	.comm	temp2i16,32,32
	.comm	mask,128,32
	.globl	ttime
	.align 8
	.type	ttime, @object
	.size	ttime, 8
ttime:
	.zero	8
	.globl	overal_time
	.data
	.align 8
	.type	overal_time, @object
	.size	overal_time, 8
overal_time:
	.quad	9999999999
	.globl	elapsed_rdtsc
	.bss
	.align 8
	.type	elapsed_rdtsc, @object
	.size	elapsed_rdtsc, 8
elapsed_rdtsc:
	.zero	8
	.comm	elapsed,8,8
	.globl	ttbest_rdtsc
	.data
	.align 8
	.type	ttbest_rdtsc, @object
	.size	ttbest_rdtsc, 8
ttbest_rdtsc:
	.quad	99999999999999999
	.comm	ttotal_rdtsc,8,8
	.comm	t2_rdtsc,8,8
	.comm	t1_rdtsc,8,8
	.comm	mask1,128,32
	.globl	programName
	.section	.rodata.str1.1
.LC38:
	.string	" "
	.data
	.align 8
	.type	programName, @object
	.size	programName, 8
programName:
	.quad	.LC38
	.globl	fileForSpeedups
	.bss
	.align 8
	.type	fileForSpeedups, @object
	.size	fileForSpeedups, 8
fileForSpeedups:
	.zero	8
	.section	.rodata.cst8,"aM",@progbits,8
	.align 8
.LC0:
	.long	2296604913
	.long	1055193269
	.align 8
.LC1:
	.long	2296604913
	.long	1056241845
	.align 8
.LC2:
	.long	2439541424
	.long	1069513965
	.section	.rodata.cst32,"aM",@progbits,32
	.align 32
.LC3:
	.long	0
	.long	1
	.long	2
	.long	3
	.long	4
	.long	5
	.long	6
	.long	7
	.align 32
.LC4:
	.long	274877907
	.long	274877907
	.long	274877907
	.long	274877907
	.long	274877907
	.long	274877907
	.long	274877907
	.long	274877907
	.align 32
.LC5:
	.byte	4
	.byte	5
	.byte	6
	.byte	7
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	12
	.byte	13
	.byte	14
	.byte	15
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	4
	.byte	5
	.byte	6
	.byte	7
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	12
	.byte	13
	.byte	14
	.byte	15
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.align 32
.LC6:
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	4
	.byte	5
	.byte	6
	.byte	7
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	12
	.byte	13
	.byte	14
	.byte	15
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	4
	.byte	5
	.byte	6
	.byte	7
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	-128
	.byte	12
	.byte	13
	.byte	14
	.byte	15
	.align 32
.LC7:
	.long	8
	.long	9
	.long	10
	.long	11
	.long	12
	.long	13
	.long	14
	.long	15
	.align 32
.LC8:
	.long	16
	.long	17
	.long	18
	.long	19
	.long	20
	.long	21
	.long	22
	.long	23
	.align 32
.LC9:
	.long	24
	.long	25
	.long	26
	.long	27
	.long	28
	.long	29
	.long	30
	.long	31
	.align 32
.LC10:
	.long	32
	.long	33
	.long	34
	.long	35
	.long	36
	.long	37
	.long	38
	.long	39
	.align 32
.LC11:
	.long	40
	.long	41
	.long	42
	.long	43
	.long	44
	.long	45
	.long	46
	.long	47
	.align 32
.LC12:
	.long	48
	.long	49
	.long	50
	.long	51
	.long	52
	.long	53
	.long	54
	.long	55
	.align 32
.LC13:
	.long	56
	.long	57
	.long	58
	.long	59
	.long	60
	.long	61
	.long	62
	.long	63
	.align 32
.LC14:
	.long	10
	.long	11
	.long	12
	.long	13
	.long	14
	.long	15
	.long	16
	.long	17
	.align 32
.LC15:
	.long	18
	.long	19
	.long	20
	.long	21
	.long	22
	.long	23
	.long	24
	.long	25
	.align 32
.LC16:
	.long	65535
	.long	65535
	.long	65535
	.long	65535
	.long	65535
	.long	65535
	.long	65535
	.long	65535
	.align 32
.LC17:
	.long	26
	.long	27
	.long	28
	.long	29
	.long	30
	.long	31
	.long	32
	.long	33
	.align 32
.LC18:
	.long	34
	.long	35
	.long	36
	.long	37
	.long	38
	.long	39
	.long	40
	.long	41
	.align 32
.LC19:
	.long	42
	.long	43
	.long	44
	.long	45
	.long	46
	.long	47
	.long	48
	.long	49
	.align 32
.LC20:
	.long	50
	.long	51
	.long	52
	.long	53
	.long	54
	.long	55
	.long	56
	.long	57
	.align 32
.LC21:
	.long	58
	.long	59
	.long	60
	.long	61
	.long	62
	.long	63
	.long	64
	.long	65
	.align 32
.LC22:
	.long	66
	.long	67
	.long	68
	.long	69
	.long	70
	.long	71
	.long	72
	.long	73
	.align 32
.LC23:
	.long	1234
	.long	1235
	.long	1236
	.long	1237
	.long	1238
	.long	1239
	.long	1240
	.long	1241
	.align 32
.LC24:
	.long	8
	.long	8
	.long	8
	.long	8
	.long	8
	.long	8
	.long	8
	.long	8
	.ident	"GCC: (GNU) 8.1.1 20180502 (Red Hat 8.1.1-1)"
	.section	.note.GNU-stack,"",@progbits
